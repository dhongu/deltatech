# Â©  2022 Deltatech
# See README.rst file on addons root folder for license details

import logging
import threading
import traceback
from io import StringIO

from psycopg2 import OperationalError

from odoo import SUPERUSER_ID, _, api, fields, models, tools
from odoo.service.model import PG_CONCURRENCY_ERRORS_TO_RETRY
from odoo.tools.safe_eval import safe_eval

from ..exception import FailedJobError, NothingToDoJob, RetryableJobError
from ..job import ENQUEUED, PENDING, Job

PG_RETRY = 5  # seconds
_logger = logging.getLogger(__name__)


class QueueJob(models.Model):
    _inherit = "queue.job"

    def run(self):
        for record in self:
            record._change_job_state(ENQUEUED)
            _logger.info("Start job: %s" % record.uuid)
            try:
                record.runjob(record.uuid)
                _logger.info("End job: %s" % record.uuid)
            except Exception:
                # new_cr.rollback()
                _logger.info("End with error job")

    def stop(self):
        for thread in threading.enumerate():
            _logger.info(thread.name)
            if thread.name == "background_job_%s" % self.id:
                _logger.info("It's still running")
                thread.join()
                # todo: de oprit rularea
        self.write({"state": "failed"})

    def background_run(self):
        threading_active = threading.active_count()
        _logger.info(threading_active)
        if threading_active > 15:
            return
        try:
            threaded_job = threading.Thread(target=self._do_run_background, args=(), name="background_job_%s" % self.id)
            threaded_job.start()
        except RuntimeError:
            pass

    def _do_run_background(self):
        # time.sleep(10)  # sa apuce sistemul sa salveze datele
        with api.Environment.manage():
            with self.pool.cursor() as new_cr:
                job = self.with_env(self.env(cr=new_cr))
                cron = job.env.ref("deltatech_queue_job.ir_cron_queue_job")
                new_cr.execute(
                    """SELECT *   FROM ir_cron   WHERE id=%s   FOR UPDATE NOWAIT""",
                    (cron.id,),
                    log_exceptions=False,
                )

                locked_job = new_cr.fetchone()
                if not locked_job:
                    _logger.debug("Job `%s` already executed by another process/thread. skipping it", cron.name)

                # job = self.with_env(self.env(cr=new_cr))
                # job._change_job_state(ENQUEUED)
                # job.runjob(job.uuid)
                # new_cr.commit()

    def _cron_runjob(self):
        self._run_job_in_threaded()
        # threaded_job = threading.Thread(target=self._run_job_in_threaded, args=(), name="queue_job")
        # threaded_job.start()

    def _run_job_in_threaded(self):
        with api.Environment.manage():
            new_cr = self.pool.cursor()
            env = api.Environment(new_cr, SUPERUSER_ID, {})
            self = self.with_env(env(cr=new_cr))

            run = True
            _logger.info("Start CRON job")
            get_param = self.env["ir.config_parameter"].sudo().get_param
            limit = safe_eval(get_param("queue_job.select_limit", "100"))

            while run:
                records = self.search([("state", "=", ENQUEUED)], order="date_created", limit=limit)  # agatate
                limit = limit - len(records)
                if limit > 0:
                    records |= self.search([("state", "=", PENDING)], order="date_created", limit=limit)

                if not records:
                    run = False
                for record in records:
                    if record.state == PENDING:
                        if record.eta and record.eta > fields.Datetime.now():
                            continue
                        record._change_job_state(ENQUEUED)
                        new_cr.commit()
                        # pylint: disable=E8102
                        # self.env.cr.commit()

                    _logger.info("Start job: %s" % record.uuid)
                    try:
                        record.runjob(record.uuid)
                        _logger.info("End job: %s" % record.uuid)
                    except Exception:
                        # new_cr.rollback()
                        _logger.info("End with error job")

                    _logger.info("Next Job")
            _logger.info("End CRON job")
            new_cr.close()

    def _try_perform_job(self, env, job):
        """Try to perform the job."""
        job.set_started()
        job.store()
        env.cr.commit()
        _logger.debug("%s started", job)

        job.perform()
        job.set_done()
        job.store()
        env["base"].flush()
        env.cr.commit()
        _logger.debug("%s done", job)

    def runjob(self, job_uuid):
        # http.request.session.db = db
        env = self.env(user=SUPERUSER_ID)

        def retry_postpone(job, message, seconds=None):
            job.env.clear()
            with api.Environment.manage():
                with self.pool.cursor() as new_cr:
                    env = api.Environment(new_cr, SUPERUSER_ID, {})
                    job.env = env
                    job.postpone(result=message, seconds=seconds)
                    job.set_pending(reset_retry=False)
                    job.store()
                    new_cr.commit()

        # ensure the job to run is in the correct state and lock the record
        env.cr.execute("SELECT state FROM queue_job WHERE uuid=%s  FOR UPDATE", (job_uuid,))
        if not env.cr.fetchone():
            _logger.warning("was requested to run job %s, but it does not exist ", job_uuid)
            return ""

        job = Job.load(env, job_uuid)
        assert job and job.state == ENQUEUED

        try:
            try:
                self._try_perform_job(env, job)
            except OperationalError as err:
                # Automatically retry the typical transaction serialization
                # errors
                if err.pgcode not in PG_CONCURRENCY_ERRORS_TO_RETRY:
                    raise

                retry_postpone(job, tools.ustr(err.pgerror, errors="replace"), seconds=PG_RETRY)
                _logger.debug("%s OperationalError, postponed", job)

        except NothingToDoJob as err:
            if str(err):
                msg = str(err)
            else:
                msg = _("Job interrupted and set to Done: nothing to do.")
            job.set_done(msg)
            job.store()
            env.cr.commit()

        except RetryableJobError as err:
            # delay the job later, requeue
            retry_postpone(job, str(err), seconds=err.seconds)
            _logger.debug("%s postponed", job)

        except (FailedJobError, Exception):
            buff = StringIO()
            traceback.print_exc(file=buff)
            traceback_txt = buff.getvalue()
            # _logger.error(traceback_txt)
            _logger.warning("Job Error")
            job.env.clear()
            with api.Environment.manage():
                with self.pool.cursor() as new_cr:
                    env = api.Environment(new_cr, SUPERUSER_ID, {})
                    job.env = env
                    job.set_failed(traceback_txt)
                    job.store()
                    new_cr.commit()
            raise Exception

        return ""
